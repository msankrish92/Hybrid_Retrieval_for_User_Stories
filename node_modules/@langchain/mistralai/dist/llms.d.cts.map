{"version":3,"file":"llms.d.cts","names":["CallbackManagerForLLMRun","BaseLLMParams","LLM","BaseLanguageModelCallOptions","GenerationChunk","LLMResult","FIMCompletionRequest","MistralAIFIMCompletionRequest","FIMCompletionStreamRequest","MistralAIFIMCompletionStreamRequest","FIMCompletionResponse","MistralAIFIMCompletionResponse","CompletionEvent","MistralAIChatCompletionEvent","BeforeRequestHook","RequestErrorHook","ResponseHook","HTTPClient","MistralAIHTTPClient","MistralAICallOptions","MistralAIInput","MistralAI","Array","Omit","Promise","AsyncIterable","AsyncGenerator"],"sources":["../src/llms.d.ts"],"sourcesContent":["import { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { BaseLLMParams, LLM } from \"@langchain/core/language_models/llms\";\nimport { type BaseLanguageModelCallOptions } from \"@langchain/core/language_models/base\";\nimport { GenerationChunk, LLMResult } from \"@langchain/core/outputs\";\nimport { FIMCompletionRequest as MistralAIFIMCompletionRequest } from \"@mistralai/mistralai/models/components/fimcompletionrequest.js\";\nimport { FIMCompletionStreamRequest as MistralAIFIMCompletionStreamRequest } from \"@mistralai/mistralai/models/components/fimcompletionstreamrequest.js\";\nimport { FIMCompletionResponse as MistralAIFIMCompletionResponse } from \"@mistralai/mistralai/models/components/fimcompletionresponse.js\";\nimport { CompletionEvent as MistralAIChatCompletionEvent } from \"@mistralai/mistralai/models/components/completionevent.js\";\nimport { BeforeRequestHook, RequestErrorHook, ResponseHook, HTTPClient as MistralAIHTTPClient } from \"@mistralai/mistralai/lib/http.js\";\nexport interface MistralAICallOptions extends BaseLanguageModelCallOptions {\n    /**\n     * Optional text/code that adds more context for the model.\n     * When given a prompt and a suffix the model will fill what\n     * is between them. When suffix is not provided, the model\n     * will simply execute completion starting with prompt.\n     */\n    suffix?: string;\n}\nexport interface MistralAIInput extends BaseLLMParams {\n    /**\n     * The name of the model to use.\n     * @default \"codestral-latest\"\n     */\n    model?: string;\n    /**\n     * The API key to use.\n     * @default {process.env.MISTRAL_API_KEY}\n     */\n    apiKey?: string;\n    /**\n     * Override the default server URL used by the Mistral SDK.\n     * @deprecated use serverURL instead\n     */\n    endpoint?: string;\n    /**\n     * Override the default server URL used by the Mistral SDK.\n     */\n    serverURL?: string;\n    /**\n     * What sampling temperature to use, between 0.0 and 2.0.\n     * Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n     * @default {0.7}\n     */\n    temperature?: number;\n    /**\n     * Nucleus sampling, where the model considers the results of the tokens with `topP` probability mass.\n     * So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n     * Should be between 0 and 1.\n     * @default {1}\n     */\n    topP?: number;\n    /**\n     * The maximum number of tokens to generate in the completion.\n     * The token count of your prompt plus maxTokens cannot exceed the model's context length.\n     */\n    maxTokens?: number;\n    /**\n     * Whether or not to stream the response.\n     * @default {false}\n     */\n    streaming?: boolean;\n    /**\n     * The seed to use for random sampling. If set, different calls will generate deterministic results.\n     * Alias for `seed`\n     */\n    randomSeed?: number;\n    /**\n     * Batch size to use when passing multiple documents to generate\n     */\n    batchSize?: number;\n    /**\n     * A list of custom hooks that must follow (req: Request) => Awaitable<Request | void>\n     * They are automatically added when a ChatMistralAI instance is created\n     */\n    beforeRequestHooks?: BeforeRequestHook[];\n    /**\n     * A list of custom hooks that must follow (err: unknown, req: Request) => Awaitable<void>\n     * They are automatically added when a ChatMistralAI instance is created\n     */\n    requestErrorHooks?: RequestErrorHook[];\n    /**\n     * A list of custom hooks that must follow (res: Response, req: Request) => Awaitable<void>\n     * They are automatically added when a ChatMistralAI instance is created\n     */\n    responseHooks?: ResponseHook[];\n    /**\n     * Optional custom HTTP client to manage API requests\n     * Allows users to add custom fetch implementations, hooks, as well as error and response processing.\n     */\n    httpClient?: MistralAIHTTPClient;\n}\n/**\n * MistralAI completions LLM.\n */\nexport declare class MistralAI extends LLM<MistralAICallOptions> implements MistralAIInput {\n    static lc_name(): string;\n    lc_namespace: string[];\n    lc_serializable: boolean;\n    model: string;\n    temperature: number;\n    topP?: number;\n    maxTokens?: number | undefined;\n    randomSeed?: number | undefined;\n    streaming: boolean;\n    batchSize: number;\n    apiKey: string;\n    /**\n     * @deprecated use serverURL instead\n     */\n    endpoint: string;\n    serverURL?: string;\n    maxRetries?: number;\n    maxConcurrency?: number;\n    beforeRequestHooks?: Array<BeforeRequestHook>;\n    requestErrorHooks?: Array<RequestErrorHook>;\n    responseHooks?: Array<ResponseHook>;\n    httpClient?: MistralAIHTTPClient;\n    constructor(fields?: MistralAIInput);\n    get lc_secrets(): {\n        [key: string]: string;\n    } | undefined;\n    get lc_aliases(): {\n        [key: string]: string;\n    } | undefined;\n    _llmType(): string;\n    invocationParams(options: this[\"ParsedCallOptions\"]): Omit<MistralAIFIMCompletionRequest | MistralAIFIMCompletionStreamRequest, \"prompt\">;\n    /**\n     * For some given input string and options, return a string output.\n     *\n     * Despite the fact that `invoke` is overridden below, we still need this\n     * in order to handle public APi calls to `generate()`.\n     */\n    _call(prompt: string, options: this[\"ParsedCallOptions\"]): Promise<string>;\n    _generate(prompts: string[], options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): Promise<LLMResult>;\n    completionWithRetry(request: MistralAIFIMCompletionRequest, options: this[\"ParsedCallOptions\"], stream: false): Promise<MistralAIFIMCompletionResponse>;\n    completionWithRetry(request: MistralAIFIMCompletionStreamRequest, options: this[\"ParsedCallOptions\"], stream: true): Promise<AsyncIterable<MistralAIChatCompletionEvent>>;\n    _streamResponseChunks(prompt: string, options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<GenerationChunk>;\n    addAllHooksToHttpClient(): void;\n    removeAllHooksFromHttpClient(): void;\n    removeHookFromHttpClient(hook: BeforeRequestHook | RequestErrorHook | ResponseHook): void;\n    private imports;\n}\n"],"mappings":";;;;;;;;;;;UASiBmB,oBAAAA,SAA6BhB;;AAA9C;AASA;;;;EA6DwC,MAKpBa,CAAAA,EAAAA,MAAAA;;AAlEoBf,UAAvBmB,cAAAA,SAAuBnB,aAAAA,CAAAA;EAAa;AA4ErD;;;EAA+D,KAmBhCa,CAAAA,EAAAA,MAAAA;EAAiB;;;;EAEV,MAAlBQ,CAAAA,EAAAA,MAAAA;EAAK;;;;EAUyG,QAAxEC,CAAAA,EAAAA,MAAAA;EAAI;;;EAQgE,SAAjBC,CAAAA,EAAAA,MAAAA;EAAO;;;;;EAEuD,WAA1CC,CAAAA,EAAAA,MAAAA;EAAa;;;;;;EAIvE,IAAGT,CAAAA,EAAAA,MAAAA;EAAY;;AA7CI;;;;;;;;;;;;;;;;;;;;;uBApBjEF;;;;;sBAKDC;;;;;kBAKJC;;;;;eAKHE;;;;;cAKIG,SAAAA,SAAkBnB,IAAIiB,iCAAiCC;;;;;;;;;;;;;;;;;;;uBAmBnDE,MAAMR;sBACPQ,MAAMP;kBACVO,MAAMN;eACTE;uBACQE;;;;;;;;wDAQiCG,KAAKhB,uBAAgCE;;;;;;;6DAOhCe;gFACmBxB,2BAA2BwB,QAAQnB;+BACpFE,0EAAmFiB,QAAQb;+BAC3FF,+EAAwFe,QAAQC,cAAcZ;yFACpDb,2BAA2B0B,eAAetB;;;iCAGlGU,oBAAoBC,mBAAmBC"}