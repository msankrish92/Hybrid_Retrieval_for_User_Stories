{"version":3,"file":"chat_models.js","names":["messages: Array<BaseMessage>","role: MessageType","content: MessageContent","type: MessageType","complex: MessageContentComplex","role: string","newContent: MistralAIContentChunk[]","message: BaseMessage","choice: NonNullable<MistralAIChatCompletionResponse[\"choices\"]>[0]","usage?: MistralAITokenUsage","rawToolCalls: MistralAIToolCall[]","uuidv4","e: any","delta: {\n    role?: string | null | undefined;\n    content?: string | MistralAIContentChunk[] | null | undefined;\n    toolCalls?: MistralAIToolCall[] | null | undefined;\n  }","usage?: MistralAITokenUsage | null","toolCallChunks: ToolCallChunk[]","tools: ChatMistralAIToolType[]","fields?: ChatMistralAIInput","options: this[\"ParsedCallOptions\"]","options?: this[\"ParsedCallOptions\"]","mistralAITools: Array<MistralAITool> | undefined","params: Omit<MistralAIChatCompletionRequest, \"messages\">","kwargs?: Partial<CallOptions>","input:\n      | MistralAIChatCompletionRequest\n      | MistralAIChatCompletionStreamRequest","streaming: boolean","MistralClient","res:\n          | MistralAIChatCompletionResponse\n          | AsyncIterable<MistralAIChatCompletionEvent>","messages: BaseMessage[]","runManager?: CallbackManagerForLLMRun","tokenUsage: TokenUsage","finalChunks: Record<number, ChatGenerationChunk>","generations","generations: ChatGeneration[]","generation: ChatGeneration","MistralAIHTTPClient","hook: BeforeRequestHook | RequestErrorHook | ResponseHook","outputSchema:\n      | InteropZodType<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>","config?: StructuredOutputMethodOptions<boolean>","schema: InteropZodType<RunOutput> | Record<string, any>","llm: Runnable<BaseLanguageModelInput>","outputParser: BaseLLMOutputParser<RunOutput>","outputSchema: JsonSchema7Type | undefined","outputSchema","openAIFunctionDefinition: FunctionDefinition","input: any","config"],"sources":["../src/chat_models.ts"],"sourcesContent":["import { v4 as uuidv4 } from \"uuid\";\nimport { Mistral as MistralClient } from \"@mistralai/mistralai\";\nimport {\n  ChatCompletionRequest as MistralAIChatCompletionRequest,\n  ChatCompletionRequestToolChoice as MistralAIToolChoice,\n  Messages as MistralAIMessage,\n} from \"@mistralai/mistralai/models/components/chatcompletionrequest.js\";\nimport { ContentChunk as MistralAIContentChunk } from \"@mistralai/mistralai/models/components/contentchunk.js\";\nimport { Tool as MistralAITool } from \"@mistralai/mistralai/models/components/tool.js\";\nimport { ToolCall as MistralAIToolCall } from \"@mistralai/mistralai/models/components/toolcall.js\";\nimport { ChatCompletionStreamRequest as MistralAIChatCompletionStreamRequest } from \"@mistralai/mistralai/models/components/chatcompletionstreamrequest.js\";\nimport { UsageInfo as MistralAITokenUsage } from \"@mistralai/mistralai/models/components/usageinfo.js\";\nimport { CompletionEvent as MistralAIChatCompletionEvent } from \"@mistralai/mistralai/models/components/completionevent.js\";\nimport { ChatCompletionResponse as MistralAIChatCompletionResponse } from \"@mistralai/mistralai/models/components/chatcompletionresponse.js\";\nimport {\n  type BeforeRequestHook,\n  type RequestErrorHook,\n  type ResponseHook,\n  HTTPClient as MistralAIHTTPClient,\n} from \"@mistralai/mistralai/lib/http.js\";\nimport {\n  BaseMessage,\n  MessageType,\n  MessageContent,\n  MessageContentComplex,\n  AIMessage,\n  HumanMessage,\n  HumanMessageChunk,\n  AIMessageChunk,\n  ToolMessageChunk,\n  ChatMessageChunk,\n  FunctionMessageChunk,\n  isAIMessage,\n} from \"@langchain/core/messages\";\nimport type {\n  BaseLanguageModelInput,\n  BaseLanguageModelCallOptions,\n  StructuredOutputMethodOptions,\n  FunctionDefinition,\n} from \"@langchain/core/language_models/base\";\nimport { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport {\n  type BaseChatModelParams,\n  BaseChatModel,\n  BindToolsInput,\n  LangSmithParams,\n} from \"@langchain/core/language_models/chat_models\";\n\nimport {\n  ChatGeneration,\n  ChatGenerationChunk,\n  ChatResult,\n} from \"@langchain/core/outputs\";\nimport { AsyncCaller } from \"@langchain/core/utils/async_caller\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\nimport { NewTokenIndices } from \"@langchain/core/callbacks/base\";\nimport {\n  type BaseLLMOutputParser,\n  JsonOutputParser,\n  StructuredOutputParser,\n} from \"@langchain/core/output_parsers\";\nimport {\n  JsonOutputKeyToolsParser,\n  convertLangChainToolCallToOpenAI,\n  makeInvalidToolCall,\n  parseToolCall,\n} from \"@langchain/core/output_parsers/openai_tools\";\nimport {\n  Runnable,\n  RunnablePassthrough,\n  RunnableSequence,\n  RunnableBinding,\n} from \"@langchain/core/runnables\";\nimport {\n  JsonSchema7Type,\n  toJsonSchema,\n} from \"@langchain/core/utils/json_schema\";\nimport { ToolCallChunk } from \"@langchain/core/messages/tool\";\nimport { isLangChainTool } from \"@langchain/core/utils/function_calling\";\nimport {\n  InteropZodType,\n  isInteropZodSchema,\n} from \"@langchain/core/utils/types\";\nimport {\n  _convertToolCallIdToMistralCompatible,\n  _mistralContentChunkToMessageContentComplex,\n} from \"./utils.js\";\n\ninterface TokenUsage {\n  completionTokens?: number;\n  promptTokens?: number;\n  totalTokens?: number;\n}\n\ntype ChatMistralAIToolType = MistralAIToolCall | MistralAITool | BindToolsInput;\n\nexport interface ChatMistralAICallOptions\n  extends Omit<BaseLanguageModelCallOptions, \"stop\"> {\n  response_format?: {\n    type: \"text\" | \"json_object\";\n  };\n  tools?: ChatMistralAIToolType[];\n  tool_choice?: MistralAIToolChoice;\n  /**\n   * Whether or not to include token usage in the stream.\n   * @default {true}\n   */\n  streamUsage?: boolean;\n}\n\n/**\n * Input to chat model class.\n */\nexport interface ChatMistralAIInput\n  extends BaseChatModelParams,\n    Pick<ChatMistralAICallOptions, \"streamUsage\"> {\n  /**\n   * The API key to use.\n   * @default {process.env.MISTRAL_API_KEY}\n   */\n  apiKey?: string;\n  /**\n   * The name of the model to use.\n   * Alias for `model`\n   * @deprecated Use `model` instead.\n   * @default {\"mistral-small-latest\"}\n   */\n  modelName?: string;\n  /**\n   * The name of the model to use.\n   * @default {\"mistral-small-latest\"}\n   */\n  model?: string;\n  /**\n   * Override the default server URL used by the Mistral SDK.\n   * @deprecated use serverURL instead\n   */\n  endpoint?: string;\n  /**\n   * Override the default server URL used by the Mistral SDK.\n   */\n  serverURL?: string;\n  /**\n   * What sampling temperature to use, between 0.0 and 2.0.\n   * Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n   * @default {0.7}\n   */\n  temperature?: number;\n  /**\n   * Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass.\n   * So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n   * Should be between 0 and 1.\n   * @default {1}\n   */\n  topP?: number;\n  /**\n   * The maximum number of tokens to generate in the completion.\n   * The token count of your prompt plus max_tokens cannot exceed the model's context length.\n   */\n  maxTokens?: number;\n  /**\n   * Whether or not to stream the response.\n   * @default {false}\n   */\n  streaming?: boolean;\n  /**\n   * Whether to inject a safety prompt before all conversations.\n   * @default {false}\n   * @deprecated use safePrompt instead\n   */\n  safeMode?: boolean;\n  /**\n   * Whether to inject a safety prompt before all conversations.\n   * @default {false}\n   */\n  safePrompt?: boolean;\n  /**\n   * The seed to use for random sampling. If set, different calls will generate deterministic results.\n   * Alias for `seed`\n   */\n  randomSeed?: number;\n  /**\n   * The seed to use for random sampling. If set, different calls will generate deterministic results.\n   */\n  seed?: number;\n  /**\n   * A list of custom hooks that must follow (req: Request) => Awaitable<Request | void>\n   * They are automatically added when a ChatMistralAI instance is created.\n   */\n  beforeRequestHooks?: BeforeRequestHook[];\n  /**\n   * A list of custom hooks that must follow (err: unknown, req: Request) => Awaitable<void>.\n   * They are automatically added when a ChatMistralAI instance is created.\n   */\n  requestErrorHooks?: RequestErrorHook[];\n  /**\n   * A list of custom hooks that must follow (res: Response, req: Request) => Awaitable<void>.\n   * They are automatically added when a ChatMistralAI instance is created.\n   */\n  responseHooks?: ResponseHook[];\n  /**\n   * Custom HTTP client to manage API requests.\n   * Allows users to add custom fetch implementations, hooks, as well as error and response processing.\n   */\n  httpClient?: MistralAIHTTPClient;\n  /**\n   * Determines how much the model penalizes the repetition of words or phrases. A higher presence\n   * penalty encourages the model to use a wider variety of words and phrases, making the output\n   * more diverse and creative.\n   */\n  presencePenalty?: number;\n  /**\n   * Penalizes the repetition of words based on their frequency in the generated text. A higher\n   * frequency penalty discourages the model from repeating words that have already appeared frequently\n   * in the output, promoting diversity and reducing repetition.\n   */\n  frequencyPenalty?: number;\n  /**\n   * Number of completions to return for each request, input tokens are only billed once.\n   */\n  numCompletions?: number;\n}\n\nexport function convertMessagesToMistralMessages(\n  messages: Array<BaseMessage>\n): Array<MistralAIMessage> {\n  const getRole = (role: MessageType) => {\n    switch (role) {\n      case \"human\":\n        return \"user\";\n      case \"ai\":\n        return \"assistant\";\n      case \"system\":\n        return \"system\";\n      case \"tool\":\n        return \"tool\";\n      case \"function\":\n        return \"assistant\";\n      default:\n        throw new Error(`Unknown message type: ${role}`);\n    }\n  };\n\n  const getContent = (\n    content: MessageContent,\n    type: MessageType\n  ): string | MistralAIContentChunk[] => {\n    const _generateContentChunk = (\n      complex: MessageContentComplex,\n      role: string\n    ): MistralAIContentChunk => {\n      if (\n        complex.type === \"image_url\" &&\n        (role === \"user\" || role === \"assistant\")\n      ) {\n        return {\n          type: complex.type,\n          imageUrl: complex?.image_url,\n        };\n      }\n\n      if (complex.type === \"text\") {\n        return {\n          type: complex.type,\n          text: complex?.text,\n        };\n      }\n\n      throw new Error(\n        `ChatMistralAI only supports messages of \"image_url\" for roles \"user\" and \"assistant\", and \"text\" for all others.\\n\\nReceived: ${JSON.stringify(\n          content,\n          null,\n          2\n        )}`\n      );\n    };\n\n    if (typeof content === \"string\") {\n      return content;\n    }\n\n    if (Array.isArray(content)) {\n      const mistralRole = getRole(type);\n      // Mistral \"assistant\" and \"user\" roles can support Mistral ContentChunks\n      // Mistral \"system\" role can support Mistral TextChunks\n      const newContent: MistralAIContentChunk[] = [];\n      content.forEach((messageContentComplex) => {\n        // Mistral content chunks only support type \"text\" and \"image_url\"\n        if (\n          messageContentComplex.type === \"text\" ||\n          messageContentComplex.type === \"image_url\"\n        ) {\n          newContent.push(\n            _generateContentChunk(messageContentComplex, mistralRole)\n          );\n        } else {\n          throw new Error(\n            `Mistral only supports types \"text\" or \"image_url\" for complex message types.`\n          );\n        }\n      });\n      return newContent;\n    }\n\n    throw new Error(\n      `Message content must be a string or an array.\\n\\nReceived: ${JSON.stringify(\n        content,\n        null,\n        2\n      )}`\n    );\n  };\n\n  const getTools = (message: BaseMessage): MistralAIToolCall[] | undefined => {\n    if (isAIMessage(message) && !!message.tool_calls?.length) {\n      return message.tool_calls\n        .map((toolCall) => ({\n          ...toolCall,\n          id: _convertToolCallIdToMistralCompatible(toolCall.id ?? \"\"),\n        }))\n        .map(convertLangChainToolCallToOpenAI) as MistralAIToolCall[];\n    }\n    return undefined;\n  };\n\n  // Build a set of toolCallIds that have corresponding tool responses present\n  // to ensure 1:1 assistant toolCalls <-> tool responses.\n  const toolResponseIds = new Set<string>();\n  for (const m of messages) {\n    if (\"tool_call_id\" in m && typeof m.tool_call_id === \"string\") {\n      toolResponseIds.add(\n        _convertToolCallIdToMistralCompatible(m.tool_call_id)\n      );\n    }\n  }\n\n  return messages.flatMap((message) => {\n    const toolCalls = getTools(message);\n    const content = getContent(message.content, message.getType());\n    if (\"tool_call_id\" in message && typeof message.tool_call_id === \"string\") {\n      return [\n        {\n          role: getRole(message.getType()),\n          content,\n          name: message.name,\n          toolCallId: _convertToolCallIdToMistralCompatible(\n            message.tool_call_id\n          ),\n        } as MistralAIMessage,\n      ];\n      // Mistral \"assistant\" role can only support either content or tool calls but not both\n    } else if (isAIMessage(message)) {\n      if (toolCalls === undefined) {\n        return [\n          {\n            role: getRole(message.getType()),\n            content,\n          } as MistralAIMessage,\n        ];\n      } else {\n        // Filter out toolCalls that do not have a matching tool response later in the list\n        const filteredToolCalls = toolCalls.filter((tc) =>\n          toolResponseIds.has(\n            _convertToolCallIdToMistralCompatible(tc.id ?? \"\")\n          )\n        );\n\n        if (filteredToolCalls.length === 0) {\n          // If there are no matching tool responses, and there's no content, drop this message\n          const isEmptyContent =\n            (typeof content === \"string\" && content.trim() === \"\") ||\n            (Array.isArray(content) && content.length === 0);\n          if (isEmptyContent) {\n            return [];\n          }\n          // Otherwise, send content only\n          return [\n            {\n              role: getRole(message.getType()),\n              content,\n            } as MistralAIMessage,\n          ];\n        }\n\n        return [\n          {\n            role: getRole(message.getType()),\n            toolCalls: filteredToolCalls as MistralAIToolCall[],\n          } as MistralAIMessage,\n        ];\n      }\n    }\n\n    return [\n      {\n        role: getRole(message.getType()),\n        content,\n      } as MistralAIMessage,\n    ];\n  }) as MistralAIMessage[];\n}\n\nfunction mistralAIResponseToChatMessage(\n  choice: NonNullable<MistralAIChatCompletionResponse[\"choices\"]>[0],\n  usage?: MistralAITokenUsage\n): BaseMessage {\n  const { message } = choice;\n  if (message === undefined) {\n    throw new Error(\"No message found in response\");\n  }\n  // MistralAI SDK does not include toolCalls in the non\n  // streaming return type, so we need to extract it like this\n  // to satisfy typescript.\n  let rawToolCalls: MistralAIToolCall[] = [];\n  if (\"toolCalls\" in message && Array.isArray(message.toolCalls)) {\n    rawToolCalls = message.toolCalls;\n  }\n  const content = _mistralContentChunkToMessageContentComplex(message.content);\n  switch (message.role) {\n    case \"assistant\": {\n      const toolCalls = [];\n      const invalidToolCalls = [];\n      for (const rawToolCall of rawToolCalls) {\n        try {\n          const parsed = parseToolCall(rawToolCall, { returnId: true });\n          toolCalls.push({\n            ...parsed,\n            id: parsed.id ?? uuidv4().replace(/-/g, \"\"),\n          });\n          // eslint-disable-next-line @typescript-eslint/no-explicit-any\n        } catch (e: any) {\n          invalidToolCalls.push(makeInvalidToolCall(rawToolCall, e.message));\n        }\n      }\n      return new AIMessage({\n        content,\n        tool_calls: toolCalls,\n        invalid_tool_calls: invalidToolCalls,\n        additional_kwargs: {},\n        usage_metadata: usage\n          ? {\n              input_tokens: usage.promptTokens,\n              output_tokens: usage.completionTokens,\n              total_tokens: usage.totalTokens,\n            }\n          : undefined,\n      });\n    }\n    default:\n      return new HumanMessage({ content });\n  }\n}\n\nfunction _convertDeltaToMessageChunk(\n  delta: {\n    role?: string | null | undefined;\n    content?: string | MistralAIContentChunk[] | null | undefined;\n    toolCalls?: MistralAIToolCall[] | null | undefined;\n  },\n  usage?: MistralAITokenUsage | null\n) {\n  if (!delta.content && !delta.toolCalls) {\n    if (usage) {\n      return new AIMessageChunk({\n        content: \"\",\n        usage_metadata: usage\n          ? {\n              input_tokens: usage.promptTokens,\n              output_tokens: usage.completionTokens,\n              total_tokens: usage.totalTokens,\n            }\n          : undefined,\n      });\n    }\n    return null;\n  }\n  // Our merge additional kwargs util function will throw unless there\n  // is an index key in each tool object (as seen in OpenAI's) so we\n  // need to insert it here.\n  const rawToolCallChunksWithIndex = delta.toolCalls?.length\n    ? delta.toolCalls?.map(\n        (toolCall, index): MistralAIToolCall & { index: number } => ({\n          ...toolCall,\n          index,\n          id: toolCall.id ?? uuidv4().replace(/-/g, \"\"),\n          type: \"function\",\n        })\n      )\n    : undefined;\n\n  let role = \"assistant\";\n  if (delta.role) {\n    role = delta.role;\n  }\n  const content = _mistralContentChunkToMessageContentComplex(delta.content);\n\n  let additional_kwargs;\n  const toolCallChunks: ToolCallChunk[] = [];\n  if (rawToolCallChunksWithIndex !== undefined) {\n    for (const rawToolCallChunk of rawToolCallChunksWithIndex) {\n      const rawArgs = rawToolCallChunk.function?.arguments;\n      const args =\n        rawArgs === undefined || typeof rawArgs === \"string\"\n          ? rawArgs\n          : JSON.stringify(rawArgs);\n      toolCallChunks.push({\n        name: rawToolCallChunk.function?.name,\n        args,\n        id: rawToolCallChunk.id,\n        index: rawToolCallChunk.index,\n        type: \"tool_call_chunk\",\n      });\n    }\n  } else {\n    additional_kwargs = {};\n  }\n\n  if (role === \"user\") {\n    return new HumanMessageChunk({ content });\n  } else if (role === \"assistant\") {\n    return new AIMessageChunk({\n      content,\n      tool_call_chunks: toolCallChunks,\n      additional_kwargs,\n      usage_metadata: usage\n        ? {\n            input_tokens: usage.promptTokens,\n            output_tokens: usage.completionTokens,\n            total_tokens: usage.totalTokens,\n          }\n        : undefined,\n    });\n  } else if (role === \"tool\") {\n    return new ToolMessageChunk({\n      content,\n      additional_kwargs,\n      tool_call_id: rawToolCallChunksWithIndex?.[0].id ?? \"\",\n    });\n  } else if (role === \"function\") {\n    return new FunctionMessageChunk({\n      content,\n      additional_kwargs,\n    });\n  } else {\n    return new ChatMessageChunk({ content, role });\n  }\n}\n\nfunction _convertToolToMistralTool(\n  tools: ChatMistralAIToolType[]\n): MistralAITool[] | undefined {\n  if (!tools || !tools.length) {\n    return undefined;\n  }\n  return tools.map((tool) => {\n    // If already a MistralAITool with a 'function' property, return as is\n    if (\"function\" in tool) {\n      return {\n        type: tool.type ?? \"function\",\n        function: tool.function,\n      };\n    }\n\n    // If it's a LangChain tool, convert to MistralAITool\n    if (isLangChainTool(tool)) {\n      const description = tool.description ?? `Tool: ${tool.name}`;\n      return {\n        type: \"function\",\n        function: {\n          name: tool.name,\n          description,\n          parameters: isInteropZodSchema(tool.schema)\n            ? toJsonSchema(tool.schema)\n            : tool.schema,\n        },\n      };\n    }\n\n    throw new Error(\n      `Unknown tool type passed to ChatMistral: ${JSON.stringify(\n        tool,\n        null,\n        2\n      )}`\n    );\n  });\n}\n\n/**\n * Mistral AI chat model integration.\n *\n * Setup:\n * Install `@langchain/mistralai` and set an environment variable named `MISTRAL_API_KEY`.\n *\n * ```bash\n * npm install @langchain/mistralai\n * export MISTRAL_API_KEY=\"your-api-key\"\n * ```\n *\n * ## [Constructor args](https://api.js.langchain.com/classes/_langchain_mistralai.ChatMistralAI.html#constructor)\n *\n * ## [Runtime args](https://api.js.langchain.com/interfaces/_langchain_mistralai.ChatMistralAICallOptions.html)\n *\n * Runtime args can be passed as the second argument to any of the base runnable methods `.invoke`. `.stream`, `.batch`, etc.\n * They can also be passed via `.withConfig`, or the second arg in `.bindTools`, like shown in the examples below:\n *\n * ```typescript\n * // When calling `.withConfig`, call options should be passed via the first argument\n * const llmWithArgsBound = llm.bindTools([...]) // tools array\n *   .withConfig({\n *     stop: [\"\\n\"], // other call options\n *   });\n *\n * // You can also bind tools and call options like this\n * const llmWithTools = llm.bindTools([...], {\n *   tool_choice: \"auto\",\n * });\n * ```\n *\n * ## Examples\n *\n * <details open>\n * <summary><strong>Instantiate</strong></summary>\n *\n * ```typescript\n * import { ChatMistralAI } from '@langchain/mistralai';\n *\n * const llm = new ChatMistralAI({\n *   model: \"mistral-large-2402\",\n *   temperature: 0,\n *   // other params...\n * });\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Invoking</strong></summary>\n *\n * ```typescript\n * const input = `Translate \"I love programming\" into French.`;\n *\n * // Models also accept a list of chat messages or a formatted prompt\n * const result = await llm.invoke(input);\n * console.log(result);\n * ```\n *\n * ```txt\n * AIMessage {\n *   \"content\": \"The translation of \\\"I love programming\\\" into French is \\\"J'aime la programmation\\\". Here's the breakdown:\\n\\n- \\\"I\\\" translates to \\\"Je\\\"\\n- \\\"love\\\" translates to \\\"aime\\\"\\n- \\\"programming\\\" translates to \\\"la programmation\\\"\\n\\nSo, \\\"J'aime la programmation\\\" means \\\"I love programming\\\" in French.\",\n *   \"additional_kwargs\": {},\n *   \"response_metadata\": {\n *     \"tokenUsage\": {\n *       \"completionTokens\": 89,\n *       \"promptTokens\": 13,\n *       \"totalTokens\": 102\n *     },\n *     \"finish_reason\": \"stop\"\n *   },\n *   \"tool_calls\": [],\n *   \"invalid_tool_calls\": [],\n *   \"usage_metadata\": {\n *     \"input_tokens\": 13,\n *     \"output_tokens\": 89,\n *     \"total_tokens\": 102\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Streaming Chunks</strong></summary>\n *\n * ```typescript\n * for await (const chunk of await llm.stream(input)) {\n *   console.log(chunk);\n * }\n * ```\n *\n * ```txt\n * AIMessageChunk {\n *   \"content\": \"The\",\n *   \"additional_kwargs\": {},\n *   \"response_metadata\": {\n *     \"prompt\": 0,\n *     \"completion\": 0\n *   },\n *   \"tool_calls\": [],\n *   \"tool_call_chunks\": [],\n *   \"invalid_tool_calls\": []\n * }\n * AIMessageChunk {\n *   \"content\": \" translation\",\n *   \"additional_kwargs\": {},\n *   \"response_metadata\": {\n *     \"prompt\": 0,\n *     \"completion\": 0\n *   },\n *   \"tool_calls\": [],\n *   \"tool_call_chunks\": [],\n *   \"invalid_tool_calls\": []\n * }\n * AIMessageChunk {\n *   \"content\": \" of\",\n *   \"additional_kwargs\": {},\n *   \"response_metadata\": {\n *     \"prompt\": 0,\n *     \"completion\": 0\n *   },\n *   \"tool_calls\": [],\n *   \"tool_call_chunks\": [],\n *   \"invalid_tool_calls\": []\n * }\n * AIMessageChunk {\n *   \"content\": \" \\\"\",\n *   \"additional_kwargs\": {},\n *   \"response_metadata\": {\n *     \"prompt\": 0,\n *     \"completion\": 0\n *   },\n *   \"tool_calls\": [],\n *   \"tool_call_chunks\": [],\n *   \"invalid_tool_calls\": []\n * }\n * AIMessageChunk {\n *   \"content\": \"I\",\n *   \"additional_kwargs\": {},\n *   \"response_metadata\": {\n *     \"prompt\": 0,\n *     \"completion\": 0\n *   },\n *   \"tool_calls\": [],\n *   \"tool_call_chunks\": [],\n *   \"invalid_tool_calls\": []\n * }\n * AIMessageChunk {\n *  \"content\": \".\",\n *  \"additional_kwargs\": {},\n *  \"response_metadata\": {\n *    \"prompt\": 0,\n *    \"completion\": 0\n *  },\n *  \"tool_calls\": [],\n *  \"tool_call_chunks\": [],\n *  \"invalid_tool_calls\": []\n *}\n *AIMessageChunk {\n *  \"content\": \"\",\n *  \"additional_kwargs\": {},\n *  \"response_metadata\": {\n *    \"prompt\": 0,\n *    \"completion\": 0\n *  },\n *  \"tool_calls\": [],\n *  \"tool_call_chunks\": [],\n *  \"invalid_tool_calls\": [],\n *  \"usage_metadata\": {\n *    \"input_tokens\": 13,\n *    \"output_tokens\": 89,\n *    \"total_tokens\": 102\n *  }\n *}\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Aggregate Streamed Chunks</strong></summary>\n *\n * ```typescript\n * import { AIMessageChunk } from '@langchain/core/messages';\n * import { concat } from '@langchain/core/utils/stream';\n *\n * const stream = await llm.stream(input);\n * let full: AIMessageChunk | undefined;\n * for await (const chunk of stream) {\n *   full = !full ? chunk : concat(full, chunk);\n * }\n * console.log(full);\n * ```\n *\n * ```txt\n * AIMessageChunk {\n *   \"content\": \"The translation of \\\"I love programming\\\" into French is \\\"J'aime la programmation\\\". Here's the breakdown:\\n\\n- \\\"I\\\" translates to \\\"Je\\\"\\n- \\\"love\\\" translates to \\\"aime\\\"\\n- \\\"programming\\\" translates to \\\"la programmation\\\"\\n\\nSo, \\\"J'aime la programmation\\\" means \\\"I love programming\\\" in French.\",\n *   \"additional_kwargs\": {},\n *   \"response_metadata\": {\n *     \"prompt\": 0,\n *     \"completion\": 0\n *   },\n *   \"tool_calls\": [],\n *   \"tool_call_chunks\": [],\n *   \"invalid_tool_calls\": [],\n *   \"usage_metadata\": {\n *     \"input_tokens\": 13,\n *     \"output_tokens\": 89,\n *     \"total_tokens\": 102\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Bind tools</strong></summary>\n *\n * ```typescript\n * import { z } from 'zod';\n *\n * const GetWeather = {\n *   name: \"GetWeather\",\n *   description: \"Get the current weather in a given location\",\n *   schema: z.object({\n *     location: z.string().describe(\"The city and state, e.g. San Francisco, CA\")\n *   }),\n * }\n *\n * const GetPopulation = {\n *   name: \"GetPopulation\",\n *   description: \"Get the current population in a given location\",\n *   schema: z.object({\n *     location: z.string().describe(\"The city and state, e.g. San Francisco, CA\")\n *   }),\n * }\n *\n * const llmWithTools = llm.bindTools([GetWeather, GetPopulation]);\n * const aiMsg = await llmWithTools.invoke(\n *   \"Which city is hotter today and which is bigger: LA or NY?\"\n * );\n * console.log(aiMsg.tool_calls);\n * ```\n *\n * ```txt\n * [\n *   {\n *     name: 'GetWeather',\n *     args: { location: 'Los Angeles, CA' },\n *     type: 'tool_call',\n *     id: '47i216yko'\n *   },\n *   {\n *     name: 'GetWeather',\n *     args: { location: 'New York, NY' },\n *     type: 'tool_call',\n *     id: 'nb3v8Fpcn'\n *   },\n *   {\n *     name: 'GetPopulation',\n *     args: { location: 'Los Angeles, CA' },\n *     type: 'tool_call',\n *     id: 'EedWzByIB'\n *   },\n *   {\n *     name: 'GetPopulation',\n *     args: { location: 'New York, NY' },\n *     type: 'tool_call',\n *     id: 'jLdLia7zC'\n *   }\n * ]\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Structured Output</strong></summary>\n *\n * ```typescript\n * import { z } from 'zod';\n *\n * const Joke = z.object({\n *   setup: z.string().describe(\"The setup of the joke\"),\n *   punchline: z.string().describe(\"The punchline to the joke\"),\n *   rating: z.number().optional().describe(\"How funny the joke is, from 1 to 10\")\n * }).describe('Joke to tell user.');\n *\n * const structuredLlm = llm.withStructuredOutput(Joke, { name: \"Joke\" });\n * const jokeResult = await structuredLlm.invoke(\"Tell me a joke about cats\");\n * console.log(jokeResult);\n * ```\n *\n * ```txt\n * {\n *   setup: \"Why don't cats play poker in the jungle?\",\n *   punchline: 'Too many cheetahs!',\n *   rating: 7\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Usage Metadata</strong></summary>\n *\n * ```typescript\n * const aiMsgForMetadata = await llm.invoke(input);\n * console.log(aiMsgForMetadata.usage_metadata);\n * ```\n *\n * ```txt\n * { input_tokens: 13, output_tokens: 89, total_tokens: 102 }\n * ```\n * </details>\n *\n * <br />\n */\nexport class ChatMistralAI<\n    CallOptions extends ChatMistralAICallOptions = ChatMistralAICallOptions\n  >\n  extends BaseChatModel<CallOptions, AIMessageChunk>\n  implements ChatMistralAIInput\n{\n  // Used for tracing, replace with the same name as your class\n  static lc_name() {\n    return \"ChatMistralAI\";\n  }\n\n  lc_namespace = [\"langchain\", \"chat_models\", \"mistralai\"];\n\n  model = \"mistral-small-latest\";\n\n  apiKey: string;\n\n  /**\n   * @deprecated use serverURL instead\n   */\n  endpoint: string;\n\n  serverURL?: string;\n\n  temperature = 0.7;\n\n  streaming = false;\n\n  topP = 1;\n\n  maxTokens: number;\n\n  /**\n   * @deprecated use safePrompt instead\n   */\n  safeMode = false;\n\n  safePrompt = false;\n\n  randomSeed?: number;\n\n  seed?: number;\n\n  maxRetries?: number;\n\n  lc_serializable = true;\n\n  streamUsage = true;\n\n  beforeRequestHooks?: Array<BeforeRequestHook>;\n\n  requestErrorHooks?: Array<RequestErrorHook>;\n\n  responseHooks?: Array<ResponseHook>;\n\n  httpClient?: MistralAIHTTPClient;\n\n  presencePenalty?: number;\n\n  frequencyPenalty?: number;\n\n  numCompletions?: number;\n\n  constructor(fields?: ChatMistralAIInput) {\n    super(fields ?? {});\n    const apiKey = fields?.apiKey ?? getEnvironmentVariable(\"MISTRAL_API_KEY\");\n    if (!apiKey) {\n      throw new Error(\n        \"API key MISTRAL_API_KEY is missing for MistralAI, but it is required.\"\n      );\n    }\n    this.apiKey = apiKey;\n    this.streaming = fields?.streaming ?? this.streaming;\n    this.serverURL = fields?.serverURL ?? this.serverURL;\n    this.temperature = fields?.temperature ?? this.temperature;\n    this.topP = fields?.topP ?? this.topP;\n    this.maxTokens = fields?.maxTokens ?? this.maxTokens;\n    this.safePrompt = fields?.safePrompt ?? this.safePrompt;\n    this.randomSeed = fields?.seed ?? fields?.randomSeed ?? this.seed;\n    this.seed = this.randomSeed;\n    this.maxRetries = fields?.maxRetries;\n    this.httpClient = fields?.httpClient;\n    this.model = fields?.model ?? fields?.modelName ?? this.model;\n    this.streamUsage = fields?.streamUsage ?? this.streamUsage;\n    this.beforeRequestHooks =\n      fields?.beforeRequestHooks ?? this.beforeRequestHooks;\n    this.requestErrorHooks =\n      fields?.requestErrorHooks ?? this.requestErrorHooks;\n    this.responseHooks = fields?.responseHooks ?? this.responseHooks;\n    this.presencePenalty = fields?.presencePenalty ?? this.presencePenalty;\n    this.frequencyPenalty = fields?.frequencyPenalty ?? this.frequencyPenalty;\n    this.numCompletions = fields?.numCompletions ?? this.numCompletions;\n    this.addAllHooksToHttpClient();\n  }\n\n  get lc_secrets(): { [key: string]: string } | undefined {\n    return {\n      apiKey: \"MISTRAL_API_KEY\",\n    };\n  }\n\n  get lc_aliases(): { [key: string]: string } | undefined {\n    return {\n      apiKey: \"mistral_api_key\",\n    };\n  }\n\n  getLsParams(options: this[\"ParsedCallOptions\"]): LangSmithParams {\n    const params = this.invocationParams(options);\n    return {\n      ls_provider: \"mistral\",\n      ls_model_name: this.model,\n      ls_model_type: \"chat\",\n      ls_temperature: params.temperature ?? undefined,\n      ls_max_tokens: params.maxTokens ?? undefined,\n    };\n  }\n\n  _llmType() {\n    return \"mistral_ai\";\n  }\n\n  /**\n   * Get the parameters used to invoke the model\n   */\n  invocationParams(\n    options?: this[\"ParsedCallOptions\"]\n  ): Omit<\n    MistralAIChatCompletionRequest | MistralAIChatCompletionStreamRequest,\n    \"messages\"\n  > {\n    const { response_format, tools, tool_choice } = options ?? {};\n    const mistralAITools: Array<MistralAITool> | undefined = tools?.length\n      ? _convertToolToMistralTool(tools)\n      : undefined;\n    const params: Omit<MistralAIChatCompletionRequest, \"messages\"> = {\n      model: this.model,\n      tools: mistralAITools,\n      temperature: this.temperature,\n      maxTokens: this.maxTokens,\n      topP: this.topP,\n      randomSeed: this.seed,\n      safePrompt: this.safePrompt,\n      toolChoice: tool_choice,\n      responseFormat: response_format,\n      presencePenalty: this.presencePenalty,\n      frequencyPenalty: this.frequencyPenalty,\n      n: this.numCompletions,\n    };\n    return params;\n  }\n\n  override bindTools(\n    tools: ChatMistralAIToolType[],\n    kwargs?: Partial<CallOptions>\n  ): Runnable<BaseLanguageModelInput, AIMessageChunk, CallOptions> {\n    const mistralTools = _convertToolToMistralTool(tools);\n    return new RunnableBinding({\n      bound: this,\n      kwargs: {\n        ...(kwargs ?? {}),\n        tools: mistralTools,\n      } as Partial<CallOptions>,\n      config: {},\n    });\n  }\n\n  /**\n   * Calls the MistralAI API with retry logic in case of failures.\n   * @param {ChatRequest} input The input to send to the MistralAI API.\n   * @returns {Promise<MistralAIChatCompletionResult | AsyncIterable<MistralAIChatCompletionEvent>>} The response from the MistralAI API.\n   */\n  async completionWithRetry(\n    input: MistralAIChatCompletionStreamRequest,\n    streaming: true\n  ): Promise<AsyncIterable<MistralAIChatCompletionEvent>>;\n\n  async completionWithRetry(\n    input: MistralAIChatCompletionRequest,\n    streaming: false\n  ): Promise<MistralAIChatCompletionResponse>;\n\n  async completionWithRetry(\n    input:\n      | MistralAIChatCompletionRequest\n      | MistralAIChatCompletionStreamRequest,\n    streaming: boolean\n  ): Promise<\n    | MistralAIChatCompletionResponse\n    | AsyncIterable<MistralAIChatCompletionEvent>\n  > {\n    const caller = new AsyncCaller({\n      maxRetries: this.maxRetries,\n    });\n    const client = new MistralClient({\n      apiKey: this.apiKey,\n      serverURL: this.serverURL,\n      // If httpClient exists, pass it into constructor\n      ...(this.httpClient ? { httpClient: this.httpClient } : {}),\n    });\n\n    return caller.call(async () => {\n      try {\n        let res:\n          | MistralAIChatCompletionResponse\n          | AsyncIterable<MistralAIChatCompletionEvent>;\n        if (streaming) {\n          res = await client.chat.stream(input);\n        } else {\n          res = await client.chat.complete(input);\n        }\n        return res;\n        // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      } catch (e: any) {\n        if (\n          e.message?.includes(\"status: 400\") ||\n          e.message?.toLowerCase().includes(\"status 400\") ||\n          e.message?.includes(\"validation failed\")\n        ) {\n          e.status = 400;\n        }\n        throw e;\n      }\n    });\n  }\n\n  /** @ignore */\n  async _generate(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<ChatResult> {\n    const tokenUsage: TokenUsage = {};\n    const params = this.invocationParams(options);\n    const mistralMessages = convertMessagesToMistralMessages(messages);\n    const input = {\n      ...params,\n      messages: mistralMessages,\n    };\n\n    // Enable streaming for signal controller or timeout due\n    // to SDK limitations on canceling requests.\n    const shouldStream = options.signal ?? !!options.timeout;\n\n    // Handle streaming\n    if (this.streaming || shouldStream) {\n      const stream = this._streamResponseChunks(messages, options, runManager);\n      const finalChunks: Record<number, ChatGenerationChunk> = {};\n      for await (const chunk of stream) {\n        const index =\n          (chunk.generationInfo as NewTokenIndices)?.completion ?? 0;\n        if (finalChunks[index] === undefined) {\n          finalChunks[index] = chunk;\n        } else {\n          finalChunks[index] = finalChunks[index].concat(chunk);\n        }\n      }\n      const generations = Object.entries(finalChunks)\n        .sort(([aKey], [bKey]) => parseInt(aKey, 10) - parseInt(bKey, 10))\n        .map(([_, value]) => value);\n\n      return { generations, llmOutput: { estimatedTokenUsage: tokenUsage } };\n    }\n\n    // Not streaming, so we can just call the API once.\n    const response = await this.completionWithRetry(input, false);\n\n    const { completionTokens, promptTokens, totalTokens } =\n      response?.usage ?? {};\n\n    if (completionTokens) {\n      tokenUsage.completionTokens =\n        (tokenUsage.completionTokens ?? 0) + completionTokens;\n    }\n\n    if (promptTokens) {\n      tokenUsage.promptTokens = (tokenUsage.promptTokens ?? 0) + promptTokens;\n    }\n\n    if (totalTokens) {\n      tokenUsage.totalTokens = (tokenUsage.totalTokens ?? 0) + totalTokens;\n    }\n\n    const generations: ChatGeneration[] = [];\n    for (const part of response?.choices ?? []) {\n      if (\"delta\" in part) {\n        throw new Error(\"Delta not supported in non-streaming mode.\");\n      }\n      if (!(\"message\" in part)) {\n        throw new Error(\"No message found in the choice.\");\n      }\n      let text = part.message?.content ?? \"\";\n      if (Array.isArray(text)) {\n        text = text[0].type === \"text\" ? text[0].text : \"\";\n      }\n      const generation: ChatGeneration = {\n        text,\n        message: mistralAIResponseToChatMessage(part, response?.usage),\n      };\n      if (part.finishReason) {\n        generation.generationInfo = { finishReason: part.finishReason };\n      }\n      generations.push(generation);\n    }\n    return {\n      generations,\n      llmOutput: { tokenUsage },\n    };\n  }\n\n  async *_streamResponseChunks(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): AsyncGenerator<ChatGenerationChunk> {\n    const mistralMessages = convertMessagesToMistralMessages(messages);\n    const params = this.invocationParams(options);\n    const input = {\n      ...params,\n      messages: mistralMessages,\n    };\n\n    const streamIterable = await this.completionWithRetry(input, true);\n    for await (const { data } of streamIterable) {\n      if (options.signal?.aborted) {\n        throw new Error(\"AbortError\");\n      }\n      const choice = data?.choices[0];\n      if (!choice || !(\"delta\" in choice)) {\n        continue;\n      }\n\n      const { delta } = choice;\n      if (!delta) {\n        continue;\n      }\n      const newTokenIndices = {\n        prompt: 0,\n        completion: choice.index ?? 0,\n      };\n      const shouldStreamUsage = this.streamUsage || options.streamUsage;\n      const message = _convertDeltaToMessageChunk(\n        delta,\n        shouldStreamUsage ? data.usage : null\n      );\n      if (message === null) {\n        // Do not yield a chunk if the message is empty\n        continue;\n      }\n      let text = delta.content ?? \"\";\n      if (Array.isArray(text)) {\n        text = text[0].type === \"text\" ? text[0].text : \"\";\n      }\n      const generationChunk = new ChatGenerationChunk({\n        message,\n        text,\n        generationInfo: newTokenIndices,\n      });\n      yield generationChunk;\n      // eslint-disable-next-line no-void\n      void runManager?.handleLLMNewToken(\n        generationChunk.text ?? \"\",\n        newTokenIndices,\n        undefined,\n        undefined,\n        undefined,\n        { chunk: generationChunk }\n      );\n    }\n  }\n\n  addAllHooksToHttpClient() {\n    try {\n      // To prevent duplicate hooks\n      this.removeAllHooksFromHttpClient();\n\n      // If the user wants to use hooks, but hasn't created an HTTPClient yet\n      const hasHooks = [\n        this.beforeRequestHooks,\n        this.requestErrorHooks,\n        this.responseHooks,\n      ].some((hook) => hook && hook.length > 0);\n      if (hasHooks && !this.httpClient) {\n        this.httpClient = new MistralAIHTTPClient();\n      }\n\n      if (this.beforeRequestHooks) {\n        for (const hook of this.beforeRequestHooks) {\n          this.httpClient?.addHook(\"beforeRequest\", hook);\n        }\n      }\n\n      if (this.requestErrorHooks) {\n        for (const hook of this.requestErrorHooks) {\n          this.httpClient?.addHook(\"requestError\", hook);\n        }\n      }\n\n      if (this.responseHooks) {\n        for (const hook of this.responseHooks) {\n          this.httpClient?.addHook(\"response\", hook);\n        }\n      }\n    } catch {\n      throw new Error(\"Error in adding all hooks\");\n    }\n  }\n\n  removeAllHooksFromHttpClient() {\n    try {\n      if (this.beforeRequestHooks) {\n        for (const hook of this.beforeRequestHooks) {\n          this.httpClient?.removeHook(\"beforeRequest\", hook);\n        }\n      }\n\n      if (this.requestErrorHooks) {\n        for (const hook of this.requestErrorHooks) {\n          this.httpClient?.removeHook(\"requestError\", hook);\n        }\n      }\n\n      if (this.responseHooks) {\n        for (const hook of this.responseHooks) {\n          this.httpClient?.removeHook(\"response\", hook);\n        }\n      }\n    } catch {\n      throw new Error(\"Error in removing hooks\");\n    }\n  }\n\n  removeHookFromHttpClient(\n    hook: BeforeRequestHook | RequestErrorHook | ResponseHook\n  ) {\n    try {\n      this.httpClient?.removeHook(\"beforeRequest\", hook as BeforeRequestHook);\n      this.httpClient?.removeHook(\"requestError\", hook as RequestErrorHook);\n      this.httpClient?.removeHook(\"response\", hook as ResponseHook);\n    } catch {\n      throw new Error(\"Error in removing hook\");\n    }\n  }\n\n  /** @ignore */\n  _combineLLMOutput() {\n    return [];\n  }\n\n  withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>\n  >(\n    outputSchema:\n      | InteropZodType<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>,\n    config?: StructuredOutputMethodOptions<false>\n  ): Runnable<BaseLanguageModelInput, RunOutput>;\n\n  withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>\n  >(\n    outputSchema:\n      | InteropZodType<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>,\n    config?: StructuredOutputMethodOptions<true>\n  ): Runnable<BaseLanguageModelInput, { raw: BaseMessage; parsed: RunOutput }>;\n\n  withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>\n  >(\n    outputSchema:\n      | InteropZodType<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>,\n    config?: StructuredOutputMethodOptions<boolean>\n  ):\n    | Runnable<BaseLanguageModelInput, RunOutput>\n    | Runnable<\n        BaseLanguageModelInput,\n        { raw: BaseMessage; parsed: RunOutput }\n      > {\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    const schema: InteropZodType<RunOutput> | Record<string, any> =\n      outputSchema;\n    const name = config?.name;\n    const method = config?.method;\n    const includeRaw = config?.includeRaw;\n\n    let llm: Runnable<BaseLanguageModelInput>;\n    let outputParser: BaseLLMOutputParser<RunOutput>;\n\n    if (method === \"jsonMode\") {\n      let outputSchema: JsonSchema7Type | undefined;\n      if (isInteropZodSchema(schema)) {\n        outputParser = StructuredOutputParser.fromZodSchema(schema);\n        outputSchema = toJsonSchema(schema);\n      } else {\n        outputParser = new JsonOutputParser<RunOutput>();\n      }\n      llm = this.withConfig({\n        response_format: { type: \"json_object\" },\n        ls_structured_output_format: {\n          kwargs: { method: \"jsonMode\" },\n          schema: outputSchema,\n        },\n      } as Partial<CallOptions>);\n    } else {\n      let functionName = name ?? \"extract\";\n      // Is function calling\n      if (isInteropZodSchema(schema)) {\n        const asJsonSchema = toJsonSchema(schema);\n        llm = this.bindTools([\n          {\n            type: \"function\" as const,\n            function: {\n              name: functionName,\n              description: asJsonSchema.description,\n              parameters: asJsonSchema,\n            },\n          },\n        ]).withConfig({\n          tool_choice: \"any\",\n          ls_structured_output_format: {\n            kwargs: { method: \"functionCalling\" },\n            schema: asJsonSchema,\n          },\n        } as Partial<CallOptions>);\n        outputParser = new JsonOutputKeyToolsParser({\n          returnSingle: true,\n          keyName: functionName,\n          zodSchema: schema,\n        });\n      } else {\n        let openAIFunctionDefinition: FunctionDefinition;\n        if (\n          typeof schema.name === \"string\" &&\n          typeof schema.parameters === \"object\" &&\n          schema.parameters != null\n        ) {\n          openAIFunctionDefinition = schema as FunctionDefinition;\n          functionName = schema.name;\n        } else {\n          openAIFunctionDefinition = {\n            name: functionName,\n            description: schema.description ?? \"\",\n            parameters: schema,\n          };\n        }\n        llm = this.bindTools([\n          {\n            type: \"function\" as const,\n            function: openAIFunctionDefinition,\n          },\n        ]).withConfig({\n          tool_choice: \"any\",\n        } as Partial<CallOptions>);\n        outputParser = new JsonOutputKeyToolsParser<RunOutput>({\n          returnSingle: true,\n          keyName: functionName,\n        });\n      }\n    }\n\n    if (!includeRaw) {\n      return llm.pipe(outputParser) as Runnable<\n        BaseLanguageModelInput,\n        RunOutput\n      >;\n    }\n\n    const parserAssign = RunnablePassthrough.assign({\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      parsed: (input: any, config) => outputParser.invoke(input.raw, config),\n    });\n    const parserNone = RunnablePassthrough.assign({\n      parsed: () => null,\n    });\n    const parsedWithFallback = parserAssign.withFallbacks({\n      fallbacks: [parserNone],\n    });\n    return RunnableSequence.from<\n      BaseLanguageModelInput,\n      { raw: BaseMessage; parsed: RunOutput }\n    >([\n      {\n        raw: llm,\n      },\n      parsedWithFallback,\n    ]);\n  }\n}\n"],"mappings":";;;;;;;;;;;;;;;;;AA+NA,SAAgB,iCACdA,UACyB;CACzB,MAAM,UAAU,CAACC,SAAsB;AACrC,UAAQ,MAAR;GACE,KAAK,QACH,QAAO;GACT,KAAK,KACH,QAAO;GACT,KAAK,SACH,QAAO;GACT,KAAK,OACH,QAAO;GACT,KAAK,WACH,QAAO;GACT,QACE,OAAM,IAAI,MAAM,CAAC,sBAAsB,EAAE,MAAM;EAClD;CACF;CAED,MAAM,aAAa,CACjBC,SACAC,SACqC;EACrC,MAAM,wBAAwB,CAC5BC,SACAC,SAC0B;AAC1B,OACE,QAAQ,SAAS,gBAChB,SAAS,UAAU,SAAS,aAE7B,QAAO;IACL,MAAM,QAAQ;IACd,UAAU,SAAS;GACpB;AAGH,OAAI,QAAQ,SAAS,OACnB,QAAO;IACL,MAAM,QAAQ;IACd,MAAM,SAAS;GAChB;AAGH,SAAM,IAAI,MACR,CAAC,8HAA8H,EAAE,KAAK,UACpI,SACA,MACA,EACD,EAAE;EAEN;AAED,MAAI,OAAO,YAAY,SACrB,QAAO;AAGT,MAAI,MAAM,QAAQ,QAAQ,EAAE;GAC1B,MAAM,cAAc,QAAQ,KAAK;GAGjC,MAAMC,aAAsC,CAAE;GAC9C,QAAQ,QAAQ,CAAC,0BAA0B;AAEzC,QACE,sBAAsB,SAAS,UAC/B,sBAAsB,SAAS,aAE/B,WAAW,KACT,sBAAsB,uBAAuB,YAAY,CAC1D;QAED,OAAM,IAAI,MACR,CAAC,4EAA4E,CAAC;GAGnF,EAAC;AACF,UAAO;EACR;AAED,QAAM,IAAI,MACR,CAAC,2DAA2D,EAAE,KAAK,UACjE,SACA,MACA,EACD,EAAE;CAEN;CAED,MAAM,WAAW,CAACC,YAA0D;AAC1E,MAAI,YAAY,QAAQ,IAAI,CAAC,CAAC,QAAQ,YAAY,OAChD,QAAO,QAAQ,WACZ,IAAI,CAAC,cAAc;GAClB,GAAG;GACH,IAAI,sCAAsC,SAAS,MAAM,GAAG;EAC7D,GAAE,CACF,IAAI,iCAAiC;AAE1C,SAAO;CACR;CAID,MAAM,kCAAkB,IAAI;AAC5B,MAAK,MAAM,KAAK,SACd,KAAI,kBAAkB,KAAK,OAAO,EAAE,iBAAiB,UACnD,gBAAgB,IACd,sCAAsC,EAAE,aAAa,CACtD;AAIL,QAAO,SAAS,QAAQ,CAAC,YAAY;EACnC,MAAM,YAAY,SAAS,QAAQ;EACnC,MAAM,UAAU,WAAW,QAAQ,SAAS,QAAQ,SAAS,CAAC;AAC9D,MAAI,kBAAkB,WAAW,OAAO,QAAQ,iBAAiB,SAC/D,QAAO,CACL;GACE,MAAM,QAAQ,QAAQ,SAAS,CAAC;GAChC;GACA,MAAM,QAAQ;GACd,YAAY,sCACV,QAAQ,aACT;EACF,CACF;WAEQ,YAAY,QAAQ,CAC7B,KAAI,cAAc,OAChB,QAAO,CACL;GACE,MAAM,QAAQ,QAAQ,SAAS,CAAC;GAChC;EACD,CACF;OACI;GAEL,MAAM,oBAAoB,UAAU,OAAO,CAAC,OAC1C,gBAAgB,IACd,sCAAsC,GAAG,MAAM,GAAG,CACnD,CACF;AAED,OAAI,kBAAkB,WAAW,GAAG;IAElC,MAAM,iBACH,OAAO,YAAY,YAAY,QAAQ,MAAM,KAAK,MAClD,MAAM,QAAQ,QAAQ,IAAI,QAAQ,WAAW;AAChD,QAAI,eACF,QAAO,CAAE;AAGX,WAAO,CACL;KACE,MAAM,QAAQ,QAAQ,SAAS,CAAC;KAChC;IACD,CACF;GACF;AAED,UAAO,CACL;IACE,MAAM,QAAQ,QAAQ,SAAS,CAAC;IAChC,WAAW;GACZ,CACF;EACF;AAGH,SAAO,CACL;GACE,MAAM,QAAQ,QAAQ,SAAS,CAAC;GAChC;EACD,CACF;CACF,EAAC;AACH;AAED,SAAS,+BACPC,QACAC,OACa;CACb,MAAM,EAAE,SAAS,GAAG;AACpB,KAAI,YAAY,OACd,OAAM,IAAI,MAAM;CAKlB,IAAIC,eAAoC,CAAE;AAC1C,KAAI,eAAe,WAAW,MAAM,QAAQ,QAAQ,UAAU,EAC5D,eAAe,QAAQ;CAEzB,MAAM,UAAU,4CAA4C,QAAQ,QAAQ;AAC5E,SAAQ,QAAQ,MAAhB;EACE,KAAK,aAAa;GAChB,MAAM,YAAY,CAAE;GACpB,MAAM,mBAAmB,CAAE;AAC3B,QAAK,MAAM,eAAe,aACxB,KAAI;IACF,MAAM,SAAS,cAAc,aAAa,EAAE,UAAU,KAAM,EAAC;IAC7D,UAAU,KAAK;KACb,GAAG;KACH,IAAI,OAAO,MAAMC,IAAQ,CAAC,QAAQ,MAAM,GAAG;IAC5C,EAAC;GAEH,SAAQC,GAAQ;IACf,iBAAiB,KAAK,oBAAoB,aAAa,EAAE,QAAQ,CAAC;GACnE;AAEH,UAAO,IAAI,UAAU;IACnB;IACA,YAAY;IACZ,oBAAoB;IACpB,mBAAmB,CAAE;IACrB,gBAAgB,QACZ;KACE,cAAc,MAAM;KACpB,eAAe,MAAM;KACrB,cAAc,MAAM;IACrB,IACD;GACL;EACF;EACD,QACE,QAAO,IAAI,aAAa,EAAE,QAAS;CACtC;AACF;AAED,SAAS,4BACPC,OAKAC,OACA;AACA,KAAI,CAAC,MAAM,WAAW,CAAC,MAAM,WAAW;AACtC,MAAI,MACF,QAAO,IAAI,eAAe;GACxB,SAAS;GACT,gBAAgB,QACZ;IACE,cAAc,MAAM;IACpB,eAAe,MAAM;IACrB,cAAc,MAAM;GACrB,IACD;EACL;AAEH,SAAO;CACR;CAID,MAAM,6BAA6B,MAAM,WAAW,SAChD,MAAM,WAAW,IACf,CAAC,UAAU,WAAkD;EAC3D,GAAG;EACH;EACA,IAAI,SAAS,MAAMH,IAAQ,CAAC,QAAQ,MAAM,GAAG;EAC7C,MAAM;CACP,GACF,GACD;CAEJ,IAAI,OAAO;AACX,KAAI,MAAM,MACR,OAAO,MAAM;CAEf,MAAM,UAAU,4CAA4C,MAAM,QAAQ;CAE1E,IAAI;CACJ,MAAMI,iBAAkC,CAAE;AAC1C,KAAI,+BAA+B,OACjC,MAAK,MAAM,oBAAoB,4BAA4B;EACzD,MAAM,UAAU,iBAAiB,UAAU;EAC3C,MAAM,OACJ,YAAY,UAAa,OAAO,YAAY,WACxC,UACA,KAAK,UAAU,QAAQ;EAC7B,eAAe,KAAK;GAClB,MAAM,iBAAiB,UAAU;GACjC;GACA,IAAI,iBAAiB;GACrB,OAAO,iBAAiB;GACxB,MAAM;EACP,EAAC;CACH;MAED,oBAAoB,CAAE;AAGxB,KAAI,SAAS,OACX,QAAO,IAAI,kBAAkB,EAAE,QAAS;UAC/B,SAAS,YAClB,QAAO,IAAI,eAAe;EACxB;EACA,kBAAkB;EAClB;EACA,gBAAgB,QACZ;GACE,cAAc,MAAM;GACpB,eAAe,MAAM;GACrB,cAAc,MAAM;EACrB,IACD;CACL;UACQ,SAAS,OAClB,QAAO,IAAI,iBAAiB;EAC1B;EACA;EACA,cAAc,6BAA6B,GAAG,MAAM;CACrD;UACQ,SAAS,WAClB,QAAO,IAAI,qBAAqB;EAC9B;EACA;CACD;KAED,QAAO,IAAI,iBAAiB;EAAE;EAAS;CAAM;AAEhD;AAED,SAAS,0BACPC,OAC6B;AAC7B,KAAI,CAAC,SAAS,CAAC,MAAM,OACnB,QAAO;AAET,QAAO,MAAM,IAAI,CAAC,SAAS;AAEzB,MAAI,cAAc,KAChB,QAAO;GACL,MAAM,KAAK,QAAQ;GACnB,UAAU,KAAK;EAChB;AAIH,MAAI,gBAAgB,KAAK,EAAE;GACzB,MAAM,cAAc,KAAK,eAAe,CAAC,MAAM,EAAE,KAAK,MAAM;AAC5D,UAAO;IACL,MAAM;IACN,UAAU;KACR,MAAM,KAAK;KACX;KACA,YAAY,mBAAmB,KAAK,OAAO,GACvC,aAAa,KAAK,OAAO,GACzB,KAAK;IACV;GACF;EACF;AAED,QAAM,IAAI,MACR,CAAC,yCAAyC,EAAE,KAAK,UAC/C,MACA,MACA,EACD,EAAE;CAEN,EAAC;AACH;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAqUD,IAAa,gBAAb,cAGU,cAEV;CAEE,OAAO,UAAU;AACf,SAAO;CACR;CAED,eAAe;EAAC;EAAa;EAAe;CAAY;CAExD,QAAQ;CAER;;;;CAKA;CAEA;CAEA,cAAc;CAEd,YAAY;CAEZ,OAAO;CAEP;;;;CAKA,WAAW;CAEX,aAAa;CAEb;CAEA;CAEA;CAEA,kBAAkB;CAElB,cAAc;CAEd;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA,YAAYC,QAA6B;EACvC,MAAM,UAAU,CAAE,EAAC;EACnB,MAAM,SAAS,QAAQ,UAAU,uBAAuB,kBAAkB;AAC1E,MAAI,CAAC,OACH,OAAM,IAAI,MACR;EAGJ,KAAK,SAAS;EACd,KAAK,YAAY,QAAQ,aAAa,KAAK;EAC3C,KAAK,YAAY,QAAQ,aAAa,KAAK;EAC3C,KAAK,cAAc,QAAQ,eAAe,KAAK;EAC/C,KAAK,OAAO,QAAQ,QAAQ,KAAK;EACjC,KAAK,YAAY,QAAQ,aAAa,KAAK;EAC3C,KAAK,aAAa,QAAQ,cAAc,KAAK;EAC7C,KAAK,aAAa,QAAQ,QAAQ,QAAQ,cAAc,KAAK;EAC7D,KAAK,OAAO,KAAK;EACjB,KAAK,aAAa,QAAQ;EAC1B,KAAK,aAAa,QAAQ;EAC1B,KAAK,QAAQ,QAAQ,SAAS,QAAQ,aAAa,KAAK;EACxD,KAAK,cAAc,QAAQ,eAAe,KAAK;EAC/C,KAAK,qBACH,QAAQ,sBAAsB,KAAK;EACrC,KAAK,oBACH,QAAQ,qBAAqB,KAAK;EACpC,KAAK,gBAAgB,QAAQ,iBAAiB,KAAK;EACnD,KAAK,kBAAkB,QAAQ,mBAAmB,KAAK;EACvD,KAAK,mBAAmB,QAAQ,oBAAoB,KAAK;EACzD,KAAK,iBAAiB,QAAQ,kBAAkB,KAAK;EACrD,KAAK,yBAAyB;CAC/B;CAED,IAAI,aAAoD;AACtD,SAAO,EACL,QAAQ,kBACT;CACF;CAED,IAAI,aAAoD;AACtD,SAAO,EACL,QAAQ,kBACT;CACF;CAED,YAAYC,SAAqD;EAC/D,MAAM,SAAS,KAAK,iBAAiB,QAAQ;AAC7C,SAAO;GACL,aAAa;GACb,eAAe,KAAK;GACpB,eAAe;GACf,gBAAgB,OAAO,eAAe;GACtC,eAAe,OAAO,aAAa;EACpC;CACF;CAED,WAAW;AACT,SAAO;CACR;;;;CAKD,iBACEC,SAIA;EACA,MAAM,EAAE,iBAAiB,OAAO,aAAa,GAAG,WAAW,CAAE;EAC7D,MAAMC,iBAAmD,OAAO,SAC5D,0BAA0B,MAAM,GAChC;EACJ,MAAMC,SAA2D;GAC/D,OAAO,KAAK;GACZ,OAAO;GACP,aAAa,KAAK;GAClB,WAAW,KAAK;GAChB,MAAM,KAAK;GACX,YAAY,KAAK;GACjB,YAAY,KAAK;GACjB,YAAY;GACZ,gBAAgB;GAChB,iBAAiB,KAAK;GACtB,kBAAkB,KAAK;GACvB,GAAG,KAAK;EACT;AACD,SAAO;CACR;CAED,AAAS,UACPL,OACAM,QAC+D;EAC/D,MAAM,eAAe,0BAA0B,MAAM;AACrD,SAAO,IAAI,gBAAgB;GACzB,OAAO;GACP,QAAQ;IACN,GAAI,UAAU,CAAE;IAChB,OAAO;GACR;GACD,QAAQ,CAAE;EACX;CACF;CAiBD,MAAM,oBACJC,OAGAC,WAIA;EACA,MAAM,SAAS,IAAI,YAAY,EAC7B,YAAY,KAAK,WAClB;EACD,MAAM,SAAS,IAAIC,QAAc;GAC/B,QAAQ,KAAK;GACb,WAAW,KAAK;GAEhB,GAAI,KAAK,aAAa,EAAE,YAAY,KAAK,WAAY,IAAG,CAAE;EAC3D;AAED,SAAO,OAAO,KAAK,YAAY;AAC7B,OAAI;IACF,IAAIC;AAGJ,QAAI,WACF,MAAM,MAAM,OAAO,KAAK,OAAO,MAAM;SAErC,MAAM,MAAM,OAAO,KAAK,SAAS,MAAM;AAEzC,WAAO;GAER,SAAQd,GAAQ;AACf,QACE,EAAE,SAAS,SAAS,cAAc,IAClC,EAAE,SAAS,aAAa,CAAC,SAAS,aAAa,IAC/C,EAAE,SAAS,SAAS,oBAAoB,EAExC,EAAE,SAAS;AAEb,UAAM;GACP;EACF,EAAC;CACH;;CAGD,MAAM,UACJe,UACAT,SACAU,YACqB;EACrB,MAAMC,aAAyB,CAAE;EACjC,MAAM,SAAS,KAAK,iBAAiB,QAAQ;EAC7C,MAAM,kBAAkB,iCAAiC,SAAS;EAClE,MAAM,QAAQ;GACZ,GAAG;GACH,UAAU;EACX;EAID,MAAM,eAAe,QAAQ,UAAU,CAAC,CAAC,QAAQ;AAGjD,MAAI,KAAK,aAAa,cAAc;GAClC,MAAM,SAAS,KAAK,sBAAsB,UAAU,SAAS,WAAW;GACxE,MAAMC,cAAmD,CAAE;AAC3D,cAAW,MAAM,SAAS,QAAQ;IAChC,MAAM,QACH,MAAM,gBAAoC,cAAc;AAC3D,QAAI,YAAY,WAAW,QACzB,YAAY,SAAS;SAErB,YAAY,SAAS,YAAY,OAAO,OAAO,MAAM;GAExD;GACD,MAAMC,gBAAc,OAAO,QAAQ,YAAY,CAC5C,KAAK,CAAC,CAAC,KAAK,EAAE,CAAC,KAAK,KAAK,SAAS,MAAM,GAAG,GAAG,SAAS,MAAM,GAAG,CAAC,CACjE,IAAI,CAAC,CAAC,GAAG,MAAM,KAAK,MAAM;AAE7B,UAAO;IAAE;IAAa,WAAW,EAAE,qBAAqB,WAAY;GAAE;EACvE;EAGD,MAAM,WAAW,MAAM,KAAK,oBAAoB,OAAO,MAAM;EAE7D,MAAM,EAAE,kBAAkB,cAAc,aAAa,GACnD,UAAU,SAAS,CAAE;AAEvB,MAAI,kBACF,WAAW,oBACR,WAAW,oBAAoB,KAAK;AAGzC,MAAI,cACF,WAAW,gBAAgB,WAAW,gBAAgB,KAAK;AAG7D,MAAI,aACF,WAAW,eAAe,WAAW,eAAe,KAAK;EAG3D,MAAMC,cAAgC,CAAE;AACxC,OAAK,MAAM,QAAQ,UAAU,WAAW,CAAE,GAAE;AAC1C,OAAI,WAAW,KACb,OAAM,IAAI,MAAM;AAElB,OAAI,EAAE,aAAa,MACjB,OAAM,IAAI,MAAM;GAElB,IAAI,OAAO,KAAK,SAAS,WAAW;AACpC,OAAI,MAAM,QAAQ,KAAK,EACrB,OAAO,KAAK,GAAG,SAAS,SAAS,KAAK,GAAG,OAAO;GAElD,MAAMC,aAA6B;IACjC;IACA,SAAS,+BAA+B,MAAM,UAAU,MAAM;GAC/D;AACD,OAAI,KAAK,cACP,WAAW,iBAAiB,EAAE,cAAc,KAAK,aAAc;GAEjE,YAAY,KAAK,WAAW;EAC7B;AACD,SAAO;GACL;GACA,WAAW,EAAE,WAAY;EAC1B;CACF;CAED,OAAO,sBACLN,UACAT,SACAU,YACqC;EACrC,MAAM,kBAAkB,iCAAiC,SAAS;EAClE,MAAM,SAAS,KAAK,iBAAiB,QAAQ;EAC7C,MAAM,QAAQ;GACZ,GAAG;GACH,UAAU;EACX;EAED,MAAM,iBAAiB,MAAM,KAAK,oBAAoB,OAAO,KAAK;AAClE,aAAW,MAAM,EAAE,MAAM,IAAI,gBAAgB;AAC3C,OAAI,QAAQ,QAAQ,QAClB,OAAM,IAAI,MAAM;GAElB,MAAM,SAAS,MAAM,QAAQ;AAC7B,OAAI,CAAC,UAAU,EAAE,WAAW,QAC1B;GAGF,MAAM,EAAE,OAAO,GAAG;AAClB,OAAI,CAAC,MACH;GAEF,MAAM,kBAAkB;IACtB,QAAQ;IACR,YAAY,OAAO,SAAS;GAC7B;GACD,MAAM,oBAAoB,KAAK,eAAe,QAAQ;GACtD,MAAM,UAAU,4BACd,OACA,oBAAoB,KAAK,QAAQ,KAClC;AACD,OAAI,YAAY,KAEd;GAEF,IAAI,OAAO,MAAM,WAAW;AAC5B,OAAI,MAAM,QAAQ,KAAK,EACrB,OAAO,KAAK,GAAG,SAAS,SAAS,KAAK,GAAG,OAAO;GAElD,MAAM,kBAAkB,IAAI,oBAAoB;IAC9C;IACA;IACA,gBAAgB;GACjB;GACD,MAAM;GAED,YAAY,kBACf,gBAAgB,QAAQ,IACxB,iBACA,QACA,QACA,QACA,EAAE,OAAO,gBAAiB,EAC3B;EACF;CACF;CAED,0BAA0B;AACxB,MAAI;GAEF,KAAK,8BAA8B;GAGnC,MAAM,WAAW;IACf,KAAK;IACL,KAAK;IACL,KAAK;GACN,EAAC,KAAK,CAAC,SAAS,QAAQ,KAAK,SAAS,EAAE;AACzC,OAAI,YAAY,CAAC,KAAK,YACpB,KAAK,aAAa,IAAIM;AAGxB,OAAI,KAAK,mBACP,MAAK,MAAM,QAAQ,KAAK,oBACtB,KAAK,YAAY,QAAQ,iBAAiB,KAAK;AAInD,OAAI,KAAK,kBACP,MAAK,MAAM,QAAQ,KAAK,mBACtB,KAAK,YAAY,QAAQ,gBAAgB,KAAK;AAIlD,OAAI,KAAK,cACP,MAAK,MAAM,QAAQ,KAAK,eACtB,KAAK,YAAY,QAAQ,YAAY,KAAK;EAG/C,QAAO;AACN,SAAM,IAAI,MAAM;EACjB;CACF;CAED,+BAA+B;AAC7B,MAAI;AACF,OAAI,KAAK,mBACP,MAAK,MAAM,QAAQ,KAAK,oBACtB,KAAK,YAAY,WAAW,iBAAiB,KAAK;AAItD,OAAI,KAAK,kBACP,MAAK,MAAM,QAAQ,KAAK,mBACtB,KAAK,YAAY,WAAW,gBAAgB,KAAK;AAIrD,OAAI,KAAK,cACP,MAAK,MAAM,QAAQ,KAAK,eACtB,KAAK,YAAY,WAAW,YAAY,KAAK;EAGlD,QAAO;AACN,SAAM,IAAI,MAAM;EACjB;CACF;CAED,yBACEC,MACA;AACA,MAAI;GACF,KAAK,YAAY,WAAW,iBAAiB,KAA0B;GACvE,KAAK,YAAY,WAAW,gBAAgB,KAAyB;GACrE,KAAK,YAAY,WAAW,YAAY,KAAqB;EAC9D,QAAO;AACN,SAAM,IAAI,MAAM;EACjB;CACF;;CAGD,oBAAoB;AAClB,SAAO,CAAE;CACV;CAwBD,qBAIEC,cAIAC,QAMI;EAEJ,MAAMC,SACJ;EACF,MAAM,OAAO,QAAQ;EACrB,MAAM,SAAS,QAAQ;EACvB,MAAM,aAAa,QAAQ;EAE3B,IAAIC;EACJ,IAAIC;AAEJ,MAAI,WAAW,YAAY;GACzB,IAAIC;AACJ,OAAI,mBAAmB,OAAO,EAAE;IAC9B,eAAe,uBAAuB,cAAc,OAAO;IAC3DC,iBAAe,aAAa,OAAO;GACpC,OACC,eAAe,IAAI;GAErB,MAAM,KAAK,WAAW;IACpB,iBAAiB,EAAE,MAAM,cAAe;IACxC,6BAA6B;KAC3B,QAAQ,EAAE,QAAQ,WAAY;KAC9B,QAAQA;IACT;GACF,EAAyB;EAC3B,OAAM;GACL,IAAI,eAAe,QAAQ;AAE3B,OAAI,mBAAmB,OAAO,EAAE;IAC9B,MAAM,eAAe,aAAa,OAAO;IACzC,MAAM,KAAK,UAAU,CACnB;KACE,MAAM;KACN,UAAU;MACR,MAAM;MACN,aAAa,aAAa;MAC1B,YAAY;KACb;IACF,CACF,EAAC,CAAC,WAAW;KACZ,aAAa;KACb,6BAA6B;MAC3B,QAAQ,EAAE,QAAQ,kBAAmB;MACrC,QAAQ;KACT;IACF,EAAyB;IAC1B,eAAe,IAAI,yBAAyB;KAC1C,cAAc;KACd,SAAS;KACT,WAAW;IACZ;GACF,OAAM;IACL,IAAIC;AACJ,QACE,OAAO,OAAO,SAAS,YACvB,OAAO,OAAO,eAAe,YAC7B,OAAO,cAAc,MACrB;KACA,2BAA2B;KAC3B,eAAe,OAAO;IACvB,OACC,2BAA2B;KACzB,MAAM;KACN,aAAa,OAAO,eAAe;KACnC,YAAY;IACb;IAEH,MAAM,KAAK,UAAU,CACnB;KACE,MAAM;KACN,UAAU;IACX,CACF,EAAC,CAAC,WAAW,EACZ,aAAa,MACd,EAAyB;IAC1B,eAAe,IAAI,yBAAoC;KACrD,cAAc;KACd,SAAS;IACV;GACF;EACF;AAED,MAAI,CAAC,WACH,QAAO,IAAI,KAAK,aAAa;EAM/B,MAAM,eAAe,oBAAoB,OAAO,EAE9C,QAAQ,CAACC,OAAYC,aAAW,aAAa,OAAO,MAAM,KAAKA,SAAO,CACvE,EAAC;EACF,MAAM,aAAa,oBAAoB,OAAO,EAC5C,QAAQ,MAAM,KACf,EAAC;EACF,MAAM,qBAAqB,aAAa,cAAc,EACpD,WAAW,CAAC,UAAW,EACxB,EAAC;AACF,SAAO,iBAAiB,KAGtB,CACA,EACE,KAAK,IACN,GACD,kBACD,EAAC;CACH;AACF"}